import pandas as pd
import numpy as np

#---------------------------------------------------------------------------------------------------
threshold = 0.7
#Dropping columns with missing value rate higher than threshold
data = data[data.columns[data.isnull().mean() < threshold]]

#Dropping rows with missing value rate higher than threshold
data = data.loc[data.isnull().mean(axis=1) < threshold]
#----------------------Numerical Imputation----------------------------------------------------
#Filling all missing values with 0
data = data.fillna(0)
#Filling missing values with medians of the columns
data = data.fillna(data.median())
#----------------------Categorical Imputation----------------------------------------------------
#Replacing the missing values with the maximum occurred value in a column
#is a good option for handling categorical columns. But if you think the values
#in the column are distributed uniformly and there is not a dominant value,
#imputing a category like “Other” might be more sensible, because in such a
#case, your imputation is likely to converge a random selection.

#Max fill function for categorical columns
data['column_name'].fillna(data['column_name'].value_counts()
.idxmax(), inplace=True)
#----------------------Outlier Detection with Standard Deviation----------------------------------------------------
#If a value has a distance to the average higher than x * standard deviation, it can be assumed as an outlier. Then what x should be?
#There is no trivial solution for x, but usually, a value between 2 and 4 seems practical.
#
##Dropping the outlier rows with standard deviation
factor = 3
upper_lim = data['column'].mean () + data['column'].std () * factor
lower_lim = data['column'].mean () - data['column'].std () * factor

data = data[(data['column'] < upper_lim) & (data['column'] > lower_lim)]

#----------------------Outlier Detection with Percentiles----------------------------------------------------
#Another mathematical method to detect outliers is to use percentiles. You
#can assume a certain percent of the value from the top or the bottom as an
#outlier. The key point is here to set the percentage value once again, and this
#depends on the distribution of your data as mentioned earlier.

#Additionally, a common mistake is using the percentiles according to the
#range of the data. In other words, if your data ranges from 0 to 100, your top
#5% is not the values between 96 and 100. Top 5% means here the values that
#are out of the 95th percentile of data.

#Dropping the outlier rows with Percentiles
upper_lim = data['column'].quantile(.95)
lower_lim = data['column'].quantile(.05)

data = data[(data['column'] < upper_lim) & (data['column'] > lower_lim)]

#----------------------An Outlier Dilemma: Drop or Cap----------------------------------------------------
#Capping the outlier rows with Percentiles
upper_lim = data['column'].quantile(.95)
lower_lim = data['column'].quantile(.05)
data.loc[(df[column] > upper_lim),column] = upper_lim
data.loc[(df[column] < lower_lim),column] = lower_lim

#----------------------Binning----------------------------------------------------
#The main motivation of binning is to make the model more robust and prevent overfitting, however, it has a cost to the performance.
#Every time you bin something, you sacrifice information and make your data more regularized. 
#(Please see regularization in machine learning)

#The trade-off between performance and overfitting is the key point of the binning process. 
#In my opinion, for numerical columns, except for some obvious overfitting cases,
#binning might be redundant for some kind of algorithms, due to its effect on model performance.

#However, for categorical columns, the labels with low frequencies probably affect the robustness of statistical models negatively.
#Thus, assigning a general category to these less frequent values helps to keep the robustness of the model. 
#For example, if your data size is 100,000 rows, it might be a good option to unite the labels with a count less than 100 to a new category like “Other”.

#Numerical Binning Example
Value      Bin       
0-30   ->  Low       
31-70  ->  Mid       
71-100 ->  High
#Categorical Binning Example
Value      Bin       
Spain  ->  Europe      
Italy  ->  Europe       
Chile  ->  South America
Brazil ->  South America

#Numerical Binning Example
data['bin'] = pd.cut(data['value'], bins=[0,30,70,100], labels=["Low", "Mid", "High"])
   value   bin
0      2   Low
1     45   Mid
2      7   Low
3     85  High
4     28   Low
#Categorical Binning Example
     Country
0      Spain
1      Chile
2  Australia
3      Italy
4     Brazil
conditions = [
    data['Country'].str.contains('Spain'),
    data['Country'].str.contains('Italy'),
    data['Country'].str.contains('Chile'),
    data['Country'].str.contains('Brazil')]

choices = ['Europe', 'Europe', 'South America', 'South America']

data['Continent'] = np.select(conditions, choices, default='Other')
     Country      Continent
0      Spain         Europe
1      Chile  South America
2  Australia          Other
3      Italy         Europe
4     Brazil  South America

#----------------------Log Transform----------------------------------------------------

Logarithm transformation (or log transform) is one of the most commonly used mathematical transformations in feature engineering. 
What are the benefits of log transform:

It helps to handle skewed data and after transformation, the distribution becomes more approximate to normal.
In most of the cases the magnitude order of the data changes within the range of the data. 
For instance, the difference between ages 15 and 20 is not equal to the ages 65 and 70. In terms of years, yes, they are identical, but for all other aspects, 5 years of difference in young ages mean a higher magnitude difference.
This type of data comes from a multiplicative process and log transform normalizes the magnitude differences like that.
It also decreases the effect of the outliers, due to the normalization of magnitude differences and the model become more robust.

A critical note: The data you apply log transform must have only positive values, otherwise you receive an error.
Also, you can add 1 to your data before transform it. Thus, you ensure the output of the transformation to be positive.

Log(x+1)


#Log Transform Example
data = pd.DataFrame({'value':[2,45, -23, 85, 28, 2, 35, -12]})
data['log+1'] = (data['value']+1).transform(np.log)
#Negative Values Handling
#Note that the values are different
data['log'] = (data['value']-data['value'].min()+1) .transform(np.log)
   value  log(x+1)  log(x-min(x)+1)
0      2   1.09861          3.25810
1     45   3.82864          4.23411
2    -23       nan          0.00000
3     85   4.45435          4.69135
4     28   3.36730          3.95124
5      2   1.09861          3.25810
6     35   3.58352          4.07754
7    -12       nan          2.48491

#----------------------One-hot encoding DUMMIES----------------------------------------------------

One-hot encoding is one of the most common encoding methods in machine learning. 
This method spreads the values in a column to multiple flag columns and assigns 0 or 1 to them. 
These binary values express the relationship between grouped and encoded column.

This method changes your categorical data, which is challenging to understand for algorithms, to a numerical format and enables you to group your categorical data without losing any information. 
(For details please see the last part of Categorical Column Grouping)


One hot encoding example on City column
Why One-Hot?: If you have N distinct values in the column, it is enough to map them to N-1 binary columns, because the missing value can be deducted from other columns.
If all the columns in our hand are equal to 0, the missing value must be equal to 1. 
This is the reason why it is called as one-hot encoding. However, I will give an example using the get_dummies function of Pandas. 
This function maps all values in a column to multiple columns.

encoded_columns = pd.get_dummies(data['column'])
data = data.join(encoded_columns).drop('column', axis=1)

#----------------------Grouping Operations----------------------------------------------------

In most machine learning algorithms, every instance is represented by a row in the training dataset, where every column show a different feature of the instance.
This kind of data called “Tidy”.

Tidy datasets are easy to manipulate, model and visualise, and have a specific structure: each variable is a column,
each observation is a row, and each type of observational unit is a table.

— Hadley Wickham

Datasets such as transactions rarely fit the definition of tidy data above, because of the multiple rows of an instance. 
In such a case, we group the data by the instances and then every instance is represented by only one row.

The key point of group by operations is to decide the aggregation functions of the features.
For numerical features, average and sum functions are usually convenient options, whereas for categorical features it more complicated.

#----------------------Categorical Column Grouping----------------------------------------------------
data.groupby('id').agg(lambda x: x.value_counts().index[0])
#Pivot table Pandas Example
data.pivot_table(index='column_to_group', columns='column_to_encode', values='aggregation_column', aggfunc=np.sum, fill_value = 0)
#----------------------Numerical Column Grouping---------------------
Numerical columns are grouped using sum and mean functions in most of the cases. 
Both can be preferable according to the meaning of the feature. For example, if you want to obtain ratio columns, you can use the average of binary columns. 
In the same example, sum function can be used to obtain the total count either.
#sum_cols: List of columns to sum
#mean_cols: List of columns to average
grouped = data.groupby('column_to_group')

sums = grouped[sum_cols].sum().add_suffix('_sum')
avgs = grouped[mean_cols].mean().add_suffix('_avg')

new_df = pd.concat([sums, avgs], axis=1)
#--------------------------Feature Split------------------------
data.name
0  Luther N. Gonzalez
1    Charles M. Young
2        Terry Lawson
3       Kristen White
4      Thomas Logsdon
#Extracting first names
data.name.str.split(" ").map(lambda x: x[0])
0     Luther
1    Charles
2      Terry
3    Kristen
4     Thomas
#Extracting last names
data.name.str.split(" ").map(lambda x: x[-1])
0    Gonzalez
1       Young
2      Lawson
3       White
4     Logsdon
#--------------------------Feature Split------------------------
#String extraction example
data.title.head()
0                      Toy Story (1995)
1                        Jumanji (1995)
2               Grumpier Old Men (1995)
3              Waiting to Exhale (1995)
4    Father of the Bride Part II (1995)
data.title.str.split("(", n=1, expand=True)[1].str.split(")", n=1, expand=True)[0]
0    1995
1    1995
2    1995
3    1995
4    1995
#--------------------------Scaling------------------------

data = pd.DataFrame({'value':[2,45, -23, 85, 28, 2, 35, -12]})

data['normalized'] = (data['value'] - data['value'].min()) / (data['value'].max() - data['value'].min())
   value  normalized
0      2        0.23
1     45        0.63
2    -23        0.00
3     85        1.00
4     28        0.47
5      2        0.23
6     35        0.54
7    -12        0.10

Standardization

Standardization (or z-score normalization) scales the values while taking into account standard deviation. 
If the standard deviation of features is different, their range also would differ from each other. 
This reduces the effect of the outliers in the features.

In the following formula of standardization, the mean is shown as μ and the standard deviation is shown as σ.

data = pd.DataFrame({'value':[2,45, -23, 85, 28, 2, 35, -12]})

data['standardized'] = (data['value'] - data['value'].mean()) / data['value'].std()
   value  standardized
0      2         -0.52
1     45          0.70
2    -23         -1.23
3     85          1.84
4     28          0.22
5      2         -0.52
6     35          0.42
7    -12         -0.92

#--------------------------Extracting Date------------------------
from datetime import date

data = pd.DataFrame({'date':
['01-01-2017',
'04-12-2008',
'23-06-1988',
'25-08-1999',
'20-02-1993',
]})

#Transform string to date
data['date'] = pd.to_datetime(data.date, format="%d-%m-%Y")

#Extracting Year
data['year'] = data['date'].dt.year

#Extracting Month
data['month'] = data['date'].dt.month

#Extracting passed years since the date
data['passed_years'] = date.today().year - data['date'].dt.year

#Extracting passed months since the date
data['passed_months'] = (date.today().year - data['date'].dt.year) * 12 + date.today().month - data['date'].dt.month

#Extracting the weekday name of the date
data['day_name'] = data['date'].dt.day_name()
        date  year  month  passed_years  passed_months   day_name
0 2017-01-01  2017      1             2             26     Sunday
1 2008-12-04  2008     12            11            123   Thursday
2 1988-06-23  1988      6            31            369   Thursday
3 1999-08-25  1999      8            20            235  Wednesday
4 1993-02-20  1993      2            26            313   Saturday
